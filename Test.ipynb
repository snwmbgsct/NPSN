{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import pickle\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from baselines.converter import get_sgcn_identity\n",
    "from npsn import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Reproducibility\n",
    "torch.manual_seed(0)\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--obs_len OBS_LEN] [--pred_len PRED_LEN]\n",
      "                             [--dataset DATASET] [--baseline BASELINE]\n",
      "                             [--batch_size BATCH_SIZE]\n",
      "                             [--num_epochs NUM_EPOCHS]\n",
      "                             [--num_samples NUM_SAMPLES]\n",
      "                             [--clip_grad CLIP_GRAD] [--lr LR]\n",
      "                             [--lr_sh_rate LR_SH_RATE] [--use_lrschd]\n",
      "                             [--tag TAG] [--gpu_num GPU_NUM]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /root/.local/share/jupyter/runtime/kernel-a23030f7-f3a1-4ad8-b14a-4bd1778f5c30.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3334: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--obs_len', type=int, default=8)\n",
    "parser.add_argument('--pred_len', type=int, default=12)\n",
    "parser.add_argument('--dataset', default='zara2', help='scene [\"eth\",\"hotel\",\"univ\",\"zara1\",\"zara2\"]')\n",
    "parser.add_argument('--baseline', default='sgcn', help='baseline network [\"sgcn\",\"stgcnn\",\"pecnet\"]')\n",
    "parser.add_argument('--batch_size', type=int, default=512, help='minibatch size')\n",
    "parser.add_argument('--num_epochs', type=int, default=2, help='number of epochs') # default: 128\n",
    "parser.add_argument('--num_samples', type=int, default=20, help='number of samples for npsn')\n",
    "parser.add_argument('--clip_grad', type=float, default=1, help='gradient clipping')\n",
    "parser.add_argument('--lr', type=float, default=0.001, help='learning rate')\n",
    "parser.add_argument('--lr_sh_rate', type=int, default=32, help='number of steps to drop the lr')\n",
    "parser.add_argument('--use_lrschd', action=\"store_true\", default=True, help='Use lr rate scheduler')\n",
    "parser.add_argument('--tag', default='npsn', help='personal tag for the model ')\n",
    "parser.add_argument('--gpu_num', default='1', type=str)\n",
    "\n",
    "args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.gpu_num\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "metrics = {'train_loss': [], 'val_loss': []}\n",
    "constant_metrics = {'min_val_epoch': -1, 'min_val_loss': 1e10}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def train(epoch, model, model_npsn, optimizer_npsn, loader_train):\n",
    "    global metrics, constant_metrics\n",
    "    model_npsn.train()\n",
    "    loss_batch = 0.\n",
    "    loader_len = len(loader_train) # 2112\n",
    "\n",
    "    for cnt, batch in enumerate(tqdm(loader_train, desc='train epoch: {}'.format(epoch), mininterval=1)): #notes: https://tqdm.github.io/\n",
    "        if cnt % args.batch_size == 0: # batch:[1,37,2,12]\n",
    "            optimizer_npsn.zero_grad()\n",
    "\n",
    "        if args.baseline == 'stgcnn':\n",
    "            V_obs, V_tr, A_obs, A_tr = data_sampler(*[batch[idx].cuda() for idx in [-4, -2, -3, -1]])\n",
    "            with torch.no_grad():\n",
    "                V_obs_tmp = V_obs.permute(0, 3, 1, 2)\n",
    "                V_pred, _ = model(V_obs_tmp, A_obs.squeeze())\n",
    "                V_pred = V_pred.permute(0, 2, 3, 1).detach()\n",
    "        elif args.baseline == 'sgcn':\n",
    "            V_obs, V_tr, _, _ = data_sampler(*[tensor.cuda() for tensor in batch[-2:]]) # *: split the tensor into individual args\n",
    "            identity = get_sgcn_identity(V_obs.shape)\n",
    "            with torch.no_grad():\n",
    "                V_pred = model(V_obs, identity).detach() # detach: duplicate a tensor detached from the current graph. no requires_grad\n",
    "            V_obs = V_obs[..., 1:]\n",
    "        elif args.baseline == 'pecnet':\n",
    "            obs_traj, pred_traj, mask, x, y, initial_pos, _ = model_forward_pre_hook(batch, data_sampler=data_sampler)\n",
    "            # NPSN\n",
    "            loc = model_npsn(obs_traj.unsqueeze(dim=0).transpose(-1, -2), mask=mask)\n",
    "            loc = loc.squeeze(dim=0).permute(1, 0, 2)\n",
    "            loc = box_muller_transform(loc)\n",
    "            # PECNet\n",
    "            all_guesses = model_forward(model, x, initial_pos, loc)\n",
    "\n",
    "        # Calculate loss\n",
    "        if args.baseline in ['stgcnn', 'sgcn']:\n",
    "            mu, cov = generate_statistics_matrices(V_pred.squeeze(dim=0))\n",
    "            loc = model_npsn(V_obs.permute(0, 2, 3, 1))\n",
    "            loss_dist, loss_disc = model_npsn.get_loss(loc, mu, cov, V_tr.permute(0, 2, 3, 1))\n",
    "        elif args.baseline == 'pecnet':\n",
    "            loss_dist, loss_disc = model_loss(all_guesses, y, loc)\n",
    "\n",
    "        loss = loss_dist * 1.0 + loss_disc * 0.01\n",
    "        loss.backward()\n",
    "        loss_batch += loss.item()\n",
    "\n",
    "        if cnt % args.batch_size + 1 == args.batch_size:  # or cnt + 1 == loader_len:  # drop last\n",
    "            if args.clip_grad is not None:\n",
    "                torch.nn.utils.clip_grad_norm_(model_npsn.parameters(), args.clip_grad)\n",
    "            optimizer_npsn.step()\n",
    "\n",
    "    metrics['train_loss'].append(loss_batch / loader_len)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def valid(epoch, model, model_npsn, checkpoint_dir, loader_val):\n",
    "    global metrics, constant_metrics\n",
    "    model_npsn.eval() # model.py-NPSN\n",
    "    loss_batch = 0.\n",
    "    loader_len = 0\n",
    "\n",
    "    for cnt, batch in enumerate(tqdm(loader_val, desc='Valid Epoch: {}'.format(epoch), mininterval=1)):\n",
    "        obs_traj, pred_traj_gt = [tensor.cuda() for tensor in batch[:2]] # len(batch[:2])=2\n",
    "\n",
    "        if args.baseline == 'stgcnn':\n",
    "            V_obs, A_obs, V_tr, A_tr = [tensor.cuda() for tensor in batch[-4:]]\n",
    "            V_obs_tmp = V_obs.permute(0, 3, 1, 2)\n",
    "            V_pred, _ = model(V_obs_tmp, A_obs.squeeze())\n",
    "            V_pred = V_pred.permute(0, 2, 3, 1)\n",
    "        elif args.baseline == 'sgcn':\n",
    "            V_obs, V_tr = [tensor.cuda() for tensor in batch[-2:]]\n",
    "            identity = get_sgcn_identity(V_obs.shape) #V_obs.shape [1, 8, 10, 3]\n",
    "            V_pred = model(V_obs, identity)\n",
    "            V_obs = V_obs[..., 1:]\n",
    "        elif args.baseline == 'pecnet':\n",
    "            obs_traj, pred_traj, mask, x, y, initial_pos, _ = model_forward_pre_hook(batch)\n",
    "            # NPSN\n",
    "            loc = model_npsn(obs_traj.unsqueeze(dim=0).transpose(-1, -2), mask=mask)\n",
    "            loc = loc.squeeze(dim=0).permute(1, 0, 2)\n",
    "            loc = box_muller_transform(loc)\n",
    "            # PECNet\n",
    "            all_guesses = model_forward(model, x, initial_pos, loc)\n",
    "\n",
    "        # Calculate metrics\n",
    "        if args.baseline in ['stgcnn', 'sgcn']:\n",
    "            mu, cov = generate_statistics_matrices(V_pred.squeeze(dim=0))\n",
    "            loc = model_npsn(V_obs.permute(0, 2, 3, 1)) # [1,10,20,2]\n",
    "\n",
    "            V_obs_traj = obs_traj.permute(0, 3, 1, 2).squeeze(dim=0) # torch.Size([8, 10, 2])\n",
    "            V_pred_traj_gt = pred_traj_gt.permute(0, 3, 1, 2).squeeze(dim=0)\n",
    "\n",
    "            # Sampling trajectories\n",
    "            V_pred_sample = purposive_sample(mu, cov, loc.size(2), loc) # torch.Size([20, 12, 10, 2])\n",
    "\n",
    "            # Evaluate trajectories\n",
    "            V_absl = V_pred_sample.cumsum(dim=1) + V_obs_traj[[-1], :, :] \n",
    "            ADEs, FDEs, TCCs = compute_batch_metric(V_absl, V_pred_traj_gt)\n",
    "\n",
    "            loss_batch += FDEs.sum().item() # item: tensor to python float\n",
    "            loader_len += FDEs.size(0)\n",
    "\n",
    "        elif args.baseline == 'pecnet':\n",
    "            loss_dist = (all_guesses - y[:, -1].unsqueeze(dim=0)).norm(p=2, dim=-1).min(dim=0)[0].sum()\n",
    "            loss_batch += loss_dist.item()\n",
    "            loader_len += loc.size(1)\n",
    "\n",
    "    metrics['val_loss'].append(loss_batch / loader_len)\n",
    "\n",
    "    if metrics['val_loss'][-1] < constant_metrics['min_val_loss']:\n",
    "        constant_metrics['min_val_loss'] = metrics['val_loss'][-1]\n",
    "        constant_metrics['min_val_epoch'] = epoch\n",
    "        torch.save(model_npsn.state_dict(), checkpoint_dir + 'val_best.pth')\n",
    "\n",
    "\n",
    "def main(args):\n",
    "    print(\"Training initiating....\")\n",
    "    print(args)\n",
    "\n",
    "    data_set = './dataset/' + args.dataset + '/' # ./dataset/zara2/\n",
    "    model_path = './pretrained/' + args.baseline + '/' + args.dataset + '/val_best.pth' # ./pretrained/sgcn/zara2/val_best.pth\n",
    "    checkpoint_dir = './checkpoints/' + args.tag + '/' + args.dataset + '/' # ./checkpoints/npsn/zara2/\n",
    "\n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        os.makedirs(checkpoint_dir)\n",
    "\n",
    "    with open(checkpoint_dir + 'args.pkl', 'wb') as fp:\n",
    "        pickle.dump(args, fp)  # https://blog.csdn.net/coffee_cream/article/details/51754484\n",
    "\n",
    "    # Dataloader\n",
    "    loader_train, _ = get_dataloader(data_set, 'train', args.obs_len, args.pred_len, args.batch_size)\n",
    "    loader_val, bs = get_dataloader(data_set, 'val', args.obs_len, args.pred_len, args.batch_size) #bs=512\n",
    "    args.batch_size = bs  # Change batch size for custom BatchSampler\n",
    "\n",
    "    # Load backbone network and NPSN\n",
    "    model = get_model().cuda()\n",
    "    model.load_state_dict(torch.load(model_path)) #'./pretrained/sgcn/zara2/val_best.pth'  state_dictionary\n",
    "    model.eval() # evaluation mode \n",
    "    model_npsn = NPSN(t_obs=args.obs_len, s=get_latent_dim(), n=args.num_samples).cuda() #NOTES: model_npsn\n",
    "    model_npsn.eval()\n",
    "    print('{} parameters:'.format(args.baseline.upper()), count_parameters(model))\n",
    "    print('NPSN parameters:', count_parameters(model_npsn))\n",
    "\n",
    "    optimizer_npsn = torch.optim.AdamW(model_npsn.parameters(), lr=args.lr)\n",
    "    if args.use_lrschd:\n",
    "        scheduler_npsn = torch.optim.lr_scheduler.StepLR(optimizer_npsn, step_size=args.lr_sh_rate, gamma=0.5)\n",
    "\n",
    "    print('Data and model loaded')\n",
    "    print('Checkpoint dir:', checkpoint_dir)\n",
    "    \n",
    "    for epoch in range(args.num_epochs):\n",
    "        train(epoch, model, model_npsn, optimizer_npsn, loader_train) #TODO\n",
    "        valid(epoch, model, model_npsn, checkpoint_dir, loader_val)\n",
    "\n",
    "        if args.use_lrschd:\n",
    "            scheduler_npsn.step()\n",
    "\n",
    "        print(\" \")\n",
    "        print(\"Dataset: {0}, Epoch: {1}\".format(args.dataset, epoch))\n",
    "        print(\"Train_loss: {0:.8f}, Val_los: {1:.8f}\".format(metrics['train_loss'][-1], metrics['val_loss'][-1]))\n",
    "        print(\"Min_val_epoch: {0}, Min_val_loss: {1:.8f}\".format(constant_metrics['min_val_epoch'],\n",
    "                                                                 constant_metrics['min_val_loss']))\n",
    "        print(\" \")\n",
    "\n",
    "        with open(checkpoint_dir + 'constant_metrics.pkl', 'wb') as fp: #'./checkpoints/npsn/zara2/constant_metrics.pkl'\n",
    "            pickle.dump(constant_metrics, fp) # pickle.dump(obj, file, protocol=None, *, fix_imports=True, buffer_callback=None)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    args = parser.parse_args()\n",
    "    main(args)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.9 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
